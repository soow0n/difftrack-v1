This repository provides tools for evaluating attention statistics, PCK, and TAP-Vid metrics on videos generated by video diffusion models such as CogVideoX and HunyuanVideo. It supports text-to-video and image-to-video generation pipelines.

Table of Contents
Setup

Directory Structure

Scripts Overview

Running Attention Statistics

Running TAP-Vid Evaluation

Example Results

## 0. Setup
```
git clone 
cd diff-track

# create conda environment (python version 3.10 is recommended)
conda create -n diff-track python=3.10
conda activate diff-track
pip install -r requirements.txt
```

## 1. Analyze Affinity Score and PCK



Directory Structure
attention_statistics.py: Evaluate attention maps, PCK, and affinity score.

evaluate_tapvid.py: Evaluate TAP-Vid tracking metrics using attention correlations.

utils/: Contains helper functions for scoring, visualization, and evaluation.

dataset/: Directory for storing prompts, video files, and ground-truth tracks.

Scripts Overview
Attention Evaluation:

Models: cogvideox_t2v, cogvideox_i2v, hunyuan_t2v

Outputs: Affinity scores, PCK metrics, attention maps, and visualized tracks.

TAP-Vid Evaluation:

Models: cogvideox_t2v, cogvideox_i2v, hunyuan_t2v

Outputs: TAP-Vid metrics, tracked trajectories.

Running Attention Statistics
Prepare your dataset:

Prompts: ./dataset/txt_prompts/foreground.txt

Index file: ./dataset/cotracker/<model>/<mode>_50.txt

Ground-truth tracks: ./dataset/cotracker/<model>/<mode>/tracks

Visibility masks: ./dataset/cotracker/<model>/<mode>/visibility

(Optional) Videos: ./dataset/videos for image-to-video pipelines.

Modify and run attention_statistics.sh:

bash
Copy
Edit
bash attention_statistics.sh
Example:

bash
Copy
Edit
model_name=cogvideox_t2v
mode=fg
gpu=0

python attention_statistics.py \
    --output_dir ./output \
    --model $model_name --video_mode $mode --num_inference_steps 50 \
    --affinity_score --affinity_mode max --pck \
    --vis_attn_map --pos_y 16 --pos_x 16 \
    --vis_track --vis_timesteps 49 --vis_layers 17 \
    --txt_path ./dataset/txt_prompts/foreground.txt \
    --idx_path ./dataset/cotracker/$model_name/${mode}_50.txt \
    --track_path ./dataset/cotracker/$model_name/$mode/tracks \
    --visibility_path ./dataset/cotracker/$model_name/$mode/visibility \
    --video_dir ./dataset/videos \
    --device cuda:$gpu --qk_device cuda:$gpu
Running TAP-Vid Evaluation
Prepare TAP-Vid dataset (DAVIS, Kinetics, RGB-Stacking) under /path/to/tapvid.

Modify and run evaluate_tapvid.sh:

bash
Copy
Edit
bash evaluate_tapvid.sh
Example:

bash
Copy
Edit
python evaluate_tapvid.py \
    --model cogvideox_t2v \
    --output_dir ./output/cogvideox_t2v \
    --eval_dataset davis_first --tapvid_root /path/to/tapvid \
    --resize_h 480 --resize_w 720 \
    --chunk_interval True --average_chunk_overlap True \
    --save_layer 33 --save_timestep 29 --inverse_step 29 \
    --pipe_device cuda:0 --qk_device cuda:0 \
    --do_inversion --inv_pipe_device cuda:0
Example Results
Attention maps saved under: ./output/XXX/attention_map/

PCK results: ./output/XXX/pck.txt and total_pck.csv

Affinity results: ./output/XXX/affinity_max.xlsx and total_affinity_max.xlsx

TAP-Vid evaluation log: ./output/cogvideox_t2v/log.txt

Notes
Set --vis_attn_map, --vis_track, --vis_video to visualize intermediate outputs.

Ensure your CUDA devices are set correctly using --device, --qk_device, --pipe_device, etc.

If using custom models, update the model paths in the script accordingly.