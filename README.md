# DiffTrack


This repository provides tools for evaluating attention statistics, PCK, and TAP-Vid metrics on videos generated by video diffusion models such as CogVideoX and HunyuanVideo. It supports text-to-video and image-to-video generation pipelines.


## Environment Settings
```
git clone
cd diff-track

conda create -n diff-track python=3.10
conda activate diff-track
pip install -r requirements.txt

cd diffusers
pip install -e .
```

# 1. Transformer Analysis
### Download Generated Dataset
For each VDM, download prompt-generated video pair dataset and pseudo ground truth trajectory, visibility dataset in `./dataset` directory. The pseudo GT dataset is producted by CoTracker3. 
```
# huggingface data 에 업로드 후 다운로드 링크 여기에
```

### Run Analysis for Generated Video

Analyze VDM in generation process. DiffTrack reproduces videos in prompt-generated video pair dataset, computing (1) matching accuracy, (2) affinity score and (3) attention score for each descriptor (query-key, intermediate feature). For more analysis with other model, please refer to `scripts/analysis` directory
```
model=cogvideox_t2v
scene=fg # bg
python analyze_generation.py \
    --output_dir ./output \
    --model $model --video_mode $scene --num_inference_steps 50 \
    --pck --affinity_score \
    --vis_timesteps 49 --vis_layers 17 \
    --vis_attn_map --pos_h 16 24 --pos_w 16 36 --vis_track \
    --txt_path ./dataset/txt_prompts/$scene.txt \
    --idx_path ./dataset/cotracker/$model/${scene}_50.txt \
    --track_path ./dataset/cotracker/$model/$scene/tracks \
    --visibility_path ./dataset/cotracker/$model/$scene/visibility \
    --device cuda:0
```

- `model` : Model for analysis. Choose from [`cogvideox_t2v_2b`, `cogvideox_t2v_5b`,`cogvideox_i2v_2b`,`cogvideox_i2v_5b`,`hunyuan_t2v`]
- `--video_mode` : Type of generated scene. Choose `fg` for object centric video or `bg` for scenary video.
- `--num_inference_steps` : The number of denoising steps. Default is 50 and  `hunyuan_t2v` model requires 30 steps.

Arguments for metric. When each arguement is stored `True`, it outputs metric for each video and averages overall metrics at the end of the code.
- `--pck` : Matching accuracy. It outputs pck for all layers/timesteps using different descriptors (query-key / intermediate feature)
- `--affinity_score` : Affinity score (cross-frame attention max) and Attention score (cross-frame attention sum). It outputs attention statistics for all layers/timesteps.
 
Arguements for visualization
- If `--vis_attn_map` is stored `True`,
    query-key and intermediate features from layers/timesteps indicated in `vis_layers`/`vis_timesteps`are used for costmap aggregation. Then query point coordinates (x, y) \in `pos_h` X `pos_w` are used for cross-frame attention visualization.
- When `--vis_track` is stored `True`,
    trajectories from query-key of `vis_layers`/`vis_timesteps` are visualized on generated video.

- `--txt_path` : Path of prompts file used for generation.
- `--idx_path` : Filepath of prompt indices used for analysis.
- `--track_path` : Path of pseudo trajectories for generated video.
- `--visibility_path` : Path of pseudo visibilities for generated video. 
- `--device` : Device for model




### Run Analysis for Real Video (TAPVID)
Analyze VDM with real video dataset(TAP-Vid-DAVIS). This analysis is only implemented on CogVideoX-2B/5B model.
```
python analyze_real.py \
    --output_dir ./output \
    --model cogvideox_t2v_2b --num_inference_steps 50 \
    --pck --affinity_score \
    --resize_h 480 --resize_w 720 \
    --eval_dataset davis_first --tapvid_root /path/to/data \
    --device cuda:0
```
</br>


# 2. Zero-Shot Point Tracking
### Download Evaluation Dataset
```
# TAP-Vid-DAVIS dataset
wget https://storage.googleapis.com/dm-tapnet/tapvid_davis.zip
unzip tapvid_davis.zip
```
For downloading TAP-Vid-Kinetics, please refer to official [TAP-Vid repository](https://github.com/google-deepmind/tapnet/tree/main/tapnet/tapvid)

### Run Evaluation
Evaluate CogVideoX-2B model with TAP-Vid-DAVIS dataset. For more evalutaion in other model and dataset, please refer to `scripts/point_tracking` directory


```
model=cogvideox_t2v_2b
python evaluate_tapvid.py \
    --model $model \
    --matching_layer 17 --matching_timestep 49 --inverse_step 49 \
    --output_dir ./output \
    --eval_dataset davis_first --tapvid_root /path/to/data \
    --resize_h 480 --resize_w 720 \
    --chunk_frame_interval --average_overlapped_corr \
    --vis_video --tracks_leave_trace 15 \
    --pipe_device cuda:0 \
```

- `model` : Model for evaluation. Choose from [`cogvideox_t2v_2b`, `cogvideox_t2v_5b`,`cogvideox_i2v_2b`,`cogvideox_i2v_5b`,`hunyuan_t2v`]

**Arguments for chunk(window)**
- `--chunk_len` : Number of frames per window(chunk). 
- `--chunk_frame_interval` : Interleave frames in window(chunk) to reduce temporal gap between global first query frame and other frames. Type is `bool`
- `--chunk_stride` : Stride for sliding window(chunk). If `chunk_frame_interval` is `True`, it becomes `1`. Default is `1`. 
- `--average_overlapped_corr` : Average correlation maps from overapped chunks. Type is `bool`


**Arguments for cost map aggregation.**
Multiple query-keys from all matching layers/timesteps will be concatenated along channel dimension and used for costmap computation.
- `--matching_layer` : Transformer layer indices where query-key are extracted. Type is `list[int]`
- `--matching_timestep` : Denoising timestep indices where query-key are extracted. Type is `list[int]`


**Arguments for dataset input**
- `--tapvid_root` : Path to dataset
- `--eval_dataset` : Type of evaluation dataset. Default is `davis_first`
- `--resize_h` : Resize height of input video. Default is `480`
- `--resize_w` : Resize width of input video. Default is `720`
- `--video_max_len` : Maximum length of input video for turncation. Default is `-1` for original input video lengths. 
- `--do_inversion` : Get inverse latent at `inverse_step`
- `--add_noise` : Add noise to latent at `inverse_step` -> none/noise/inversion 으로 코드 수정하기

**Args for trajectory visualization**
- `--vis_video` : Visualize predicted trajectories on video. Type is `bool`
- `--tracks_leave_trace` : Frame length of trace line when visualizing trajectories. Default is `15`

</br>


# 3. Cross Attention Guidance (CAG)
This experiment is only implemented on CogVideoX-2B/5B model.
```
# pag -> cag로 명칭 바꾸기
```
- `--pag_layers` : Select layers where CAG is offered. We recommand set this as top-3 layers from Transformer Analysis. `[13, 17, 21]` for CogVideoX-2B and `[15, 17, 18]` for CogVidoeX-5B.
- `--pag_timestep` 
- `--cag_scale` : Scale of CAG. Type is `float` and default is `1.0`
- `--cfg_scale` : Scale of CFG(Classifier-Free Guidance). Type is `float` and default is `6.0`

