This repository provides tools for evaluating attention statistics, PCK, and TAP-Vid metrics on videos generated by video diffusion models such as CogVideoX and HunyuanVideo. It supports text-to-video and image-to-video generation pipelines.


# Environment Settings
```
git clone
cd diff-track

conda create -n diff-track python=3.10
conda activate diff-track
pip install -r requirements.txt

cd diffusers
pip install -e .
```

# Zero-Shot Point Tracking
### 1. Download Evaluation Dataset
```
# TAP-Vid-DAVIS dataset
wget https://storage.googleapis.com/dm-tapnet/tapvid_davis.zip
unzip tapvid_davis.zip
```
For downloading TAP-Vid-Kinetics, please refer to official [TAP-Vid repository](https://github.com/google-deepmind/tapnet/tree/main/tapnet/tapvid)

### 2. Run Evaluation
Evaluate CogVideoX-2B model with TAP-Vid-DAVIS dataset. For more evalutaion in other model and dataset, please refer to `scripts/evaluation` directory


```
model=cogvideox_t2v
python evaluate_tapvid.py \
    --model $model \
    --matching_layer 17 --matching_timestep 49 --inverse_step 49 \
    --output_dir ./output \
    --eval_dataset davis_first --tapvid_root /path/to/data \
    --resize_h 480 --resize_w 720 \
    --chunk_frame_interval --average_overlapped_corr \
    --vis_video --tracks_leave_trace 15 \
    --pipe_device cuda:0 \
```

- `model` : Model for evaluation. Default is `cogvideox_t2v`
- `--chunk_len` : Number of frames per window(chunk). 
- `--chunk_frame_interval` : Interleave frames in window(chunk) to reduce temporal gap between global first query frame and other frames. Type is `bool`
- `chunk_stride` : Stride for sliding window(chunk). If `chunk_frame_interval` is `True`, it becomes `1`. Default is `1`. 

- `--matching_layer` : Transformer layer indices where query-key are extracted. Type is `list[int]`

- `--matching_timestep` : Denoising timestep indices where query-key are extracted. Type is `list[int]`

    Multiple query-keys from all matching layers/timesteps will be concatenated along channel dimension and used for costmap computation.
- `--tapvid_root` : Path to dataset
- `--eval_dataset` : Type of evaluation dataset. Default is `davis_first`
- `--resize_h` : Resize height of input video. Default is `480`
- `--resize_w` : Resize width of input video. Default is `720`

- `--average_overlapped_corr` : Average correlation map in overapped chunks. Type is `bool`
- `--video_max_len` : Maximum length of input video for turncation. Default is `-1` for original input video lengths. 
- `--do_inversion` : Get inverse latent at tims `inverse_step`
- `--vis_video` : Visualize predicted trajectories on video. Type is `bool`
- `--tracks_leave_trace` : Frame length of trace line when visualizing trajectories. Default is `15`

</br>

# Attention Analysis
### 1. Download Generated Dataset
Download prompt-generated video pair dataset and pseudo ground truth trajectory, visibility dataset in `./dataset` directory. The pseudo GT dataset is producted by CoTracker3. 
```

```

### 2. Run Analysis for Generated Video

Analyze VDM in generation process. It reproduces videos in prompt-generated video pair dataset, compute affinity score and temporal accuracy(pck) for each descriptor (query-key, query-key head and intermediate feature). For more analysis with other model, please refer to `scripts/analysis` directory
```
model=cogvideox_t2v
scene=fg # bg
python analyze_generation.py \
    --output_dir ./output \
    --model $model --video_mode $scene --num_inference_steps 50 \
    --pck --affinity_score \
    --vis_timesteps 49 --vis_layers 17 \
    --vis_attn_map --pos_h 16 24 --pos_w 16 36 --vis_track \
    --txt_path ./dataset/txt_prompts/$scene.txt \
    --idx_path ./dataset/cotracker/$model/${scene}_50.txt \
    --track_path ./dataset/cotracker/$model/$scene/tracks \
    --visibility_path ./dataset/cotracker/$model/$scene/visibility \
    --device cuda:0
```



### 3. Run Analysis for Real Video (TAPVID)
Analyze VDM with real video dataset(TAP-Vid-DAVIS, TAP-Vid-Kinetics). This analysis is only implemented on CogVideoX-2B model.
```
python analyze_real.py \
    --output_dir ./output \
    --model cogvideox_t2v --num_inference_steps 50 \
    --pck --affinity_score \
    --resize_h 480 --resize_w 720 \
    --eval_dataset davis_first --tapvid_root /path/to/data \
    --device cuda:0

```
