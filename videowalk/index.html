
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-67835289-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-67835289-3');
</script>

<!-- ======================================================================= -->
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<style type="text/css">
  body {
    /* font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; */
    font-family: "Avenir", sans-serif;
    font-weight:400;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 1100px;
  }

  h1 {
    font-weight:300;
  }

  .column {
    float: left;
    /* width: 50%; */
  }

  /* Clear floats after the columns */
  .row:after {
    content: "";
    display: table;
    clear: both;
  }
  
  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }

  #authors td {
    padding-bottom:5px;
    padding-top:30px;
  }
</style>


<html>
  <head>

	<title>Space-Time Correspondence as a Contrastive Random Walk</title>
	<meta property="og:title" content="Space-Time Correspondence as a Contrastive Random Walk " />
	<meta property="og:image" content="" />
	<meta property="og:image:width" content="3000" />
	<meta property="og:image:height" content="800" />
  </head>

  <body>
    <br><br><br><br>
          <center>
          	<span style="font-size:38px">Space-Time Correspondence as a Contrastive Random Walk</span>
            <br><br>
		  <table align=center width=600px>
	  			  <tr>
	  	              <td align=center width=100px>
	  					<center>
	  						<span style="font-size:24px"><a href="http://ajabri.github.io">Allan A. Jabri</a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=100px>
              <center>
	  						<span style="font-size:24px"><a href="http://www.andrewowens.com">Andrew Owens</a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=100px>
	  					<center>
	  						<span style="font-size:24px"><a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a></span>
		  		  		</center>
		  		  	  </td>
			  </table>

	  		  <table align=center width=650px style="font-size:20px;">
	  			  <tr>
              <td align=center width=150px>
                <center>
                  <span><a href="https://proceedings.neurips.cc/paper/2020/hash/e2ef524fbf3d9fe611d5a8e90fefdc9c-Abstract.html"> [Paper]</a></span>
                  </center>
                  </td>
	  	          <td align=center width=150px>
	  					<center>
	  						<span style=""><a href="http://www.github.com/ajabri/videowalk"> [Code]</a></span>
		  		  		</center>
		  		  	  </td>
                <td align=center width=150px>
	  					<center>
	  						<span style=""><a href="https://youtu.be/UaOcjxrPaho"> [Talk]</a></span>
		  		  		</center>
		  		  	  </td>
	  	          <td align=center width=150px>
	  					<center>
	  						<span style=""><a href="https://www.dropbox.com/s/snpj68cssu3b4to/jabri.neurips2020.poster.pdf"> [Poster]</a></span>
		  		  		</center>
		  		  	  </td>
	  	          <td align=center width=150px>
                  <center>
                    <span style=""><a href="https://www.dropbox.com/s/qrqb0ssjlh1tph1/jabri.nips.12min.public.key"> [Slides]</a></span>
                    </center>
                  </td>
              </tr>
	  			  <tr>
			  </table>
        <br>
        <span style="font-size:28px">NeurIPS 2020</span>
      </center>

  		  <br><br>
  		  <table align=center width=850px>
  			  <tr>
            <td width=400px>
    					<center>
    	                	<a href="figs/teaser_animation.gif"><img src = "figs/teaser_animation.gif" width="90%"></img></href></a><br>
  					  </center>
            </td>
          </tr>
  		  </table>
      	<br><br>

		  <hr>

  		  <table align=center>
          <div class='row'>
            <div class='column'>
              <center>
                <h1>Abstract</h1>
                <p style="width:560px; text-align:left; color:darkslategray; ;">This paper proposes a simple self-supervised approach for learning representations
                  for visual correspondence from raw video. We cast correspondence as link prediction in a space-time graph constructed from a video. In this graph, the nodes
                  are patches sampled from each frame, and nodes adjacent in time can share a
                  directed edge. We learn a node embedding in which pairwise similarity defines
                  transition probabilities of a random walk. Prediction of long-range correspondence
                  is efficiently computed as a walk along this graph. The embedding learns to guide
                  the walk by placing high probability along paths of correspondence. Targets are
                  formed without supervision, by cycle-consistency: we train the embedding to maximize the likelihood of returning to the initial node when walking along a graph
                  constructed from a 'palindrome' of frames. We demonstrate that the approach allows
                  for learning representations from large unlabeled video. Despite its simplicity,
                  the method outperforms the self-supervised state-of-the-art on a variety of label
                  propagation tasks involving objects, semantic parts, and pose. Moreover, we show
                  that self-supervised adaptation at test-time and edge dropout improve transfer for
                  object-level correspondence.</p>
              </center>
            </div>

            <div class="column" style='float:right; width:40%'>
              <center>
                <h1>Pseudo-code</h1>
                <!-- <embed src="figs/pseudocode.pdf" width="800px" height="2100px" /> -->
                <img src = "figs/pseudocode.png" width="90%"></img>
              </center>
            </div>
          </div>
        </table>
        <br><br>
		  </table>

		  <hr>

      <center>
        <h1>Talk</h1>
        <table border="0" align="center" cellspacing="0" cellpadding="20">
            <td align="center" valign="middle">
            <iframe width="800" height="450" src="https://www.youtube.com/embed/UaOcjxrPaho" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </td>
        </table>
      </center>
      <hr>
      <center>
      <h1>More Qualitative Results</h1>
      <table border="0" align="center" cellspacing="0" cellpadding="20">
          <td align="center" valign="middle">
          <iframe width="800" height="450" src="https://www.youtube.com/embed/R_Zae5N_hKw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </td>
      </table>
      </center>


      <hr>
  		  <!-- <table align=center width=550px> -->
  		  <table align=center width=800px>
	 		<center><h1>Paper</h1></center>
  			  <tr>
				  <td><a href=""><img class="layered-paper-big" style="height:175px" src="./figs/firstpage.png"/></a></td>
				  <td><span style="font-size:14pt">Allan Jabri, Andrew Owens, Alexei A. Efros.<br>
				  <i><a href="https://proceedings.neurips.cc/paper/2020/hash/e2ef524fbf3d9fe611d5a8e90fefdc9c-Abstract.html">Space-Time Correspondence as a Contrastive Random Walk.</a></i><br>
				  NeurIPS 2020, Oral Presentation.<br>
				  </a>
          <span style="font-size:4pt"><a href=""><br></a>
				  </span>
				  </td>
  	              </td>
              </tr>
  		  </table>
		  <br>

		  <table align=center width=600px>
			  <tr>
				  <td><span style="font-size:14pt"><center>
				  	<a href="./figs/bibtext.txt">[Bibtex]</a>
  	              </center></td>
              </tr>
  		  </table>

        <hr>

  		  <table align=center width=1100px>
  			  <tr>
  	              <td width=400px>
  					<left>
          <center><h1>Acknowledgements</h1></center>
          We thank Amir Zamir, Ashish Kumar, Tim Brooks, Bill Peebles, Dave Epstein,
          Armand Joulin, and Jitendra Malik for very helpful feedback. We are also grateful to the wonderful
          members of VGG for hosting us during a dreamy semester at Oxford. This work would not
          have been possible without the hospitality of Port Meadow and the swimming pool on Iffley Road.
          Research was supported, in part, by NSF grant IIS-1633310, the DARPA MCS program, and NSF
          IIS-1522904. AJ is supported by the PD Soros Fellowship.

        </table>
    		  <br><br>
  		  </table>

			</left>
		</td>
			 </tr>
		</table>

		<br><br>


</body>
</html>
